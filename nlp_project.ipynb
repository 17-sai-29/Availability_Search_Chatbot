{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import math\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCMULamP5R9g",
        "outputId": "55fbd3bd-46ba-4ffb-d5f2-17a94ecf2097"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxWWp6hCqXbO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from collections import defaultdict\n",
        "import math\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "\n",
        "class NaiveBayesFromScratch:\n",
        "    def __init__(self):\n",
        "        self.class_priors = defaultdict(float)\n",
        "        self.feature_probs = defaultdict(lambda: defaultdict(float))\n",
        "        self.vocabulary = set()\n",
        "        self.total_samples = 0\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return text.lower().split()\n",
        "\n",
        "    def train(self, labeled_data):\n",
        "        label_counts = defaultdict(int)\n",
        "        word_given_label_counts = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "        for features, label in labeled_data:\n",
        "            label_counts[label] += 1\n",
        "            tokens = self.tokenize(features['book'])\n",
        "            self.vocabulary.update(tokens)\n",
        "            for word in tokens:\n",
        "                word_given_label_counts[label][word] += 1\n",
        "\n",
        "        self.total_samples = sum(label_counts.values())\n",
        "        vocab_size = len(self.vocabulary)\n",
        "\n",
        "        for label in label_counts:\n",
        "            self.class_priors[label] = label_counts[label] / self.total_samples\n",
        "\n",
        "        for label in label_counts:\n",
        "            total_words = sum(word_given_label_counts[label].values())\n",
        "            for word in self.vocabulary:\n",
        "                word_count = word_given_label_counts[label][word]\n",
        "                self.feature_probs[label][word] = (word_count + 1) / (total_words + vocab_size)\n",
        "\n",
        "    def classify(self, features):\n",
        "        tokens = self.tokenize(features['book'])\n",
        "        scores = {}\n",
        "\n",
        "        for label in self.class_priors:\n",
        "            log_prob = math.log(self.class_priors[label])\n",
        "            for word in tokens:\n",
        "                if word in self.vocabulary:\n",
        "                    log_prob += math.log(self.feature_probs[label][word])\n",
        "            scores[label] = log_prob\n",
        "\n",
        "        return max(scores, key=scores.get)\n",
        "\n",
        "\n",
        "\n",
        "def extract_nouns(tokens):\n",
        "    tagged_tokens = pos_tag(tokens)\n",
        "    nouns = [word for word, pos in tagged_tokens if pos.startswith('N')]\n",
        "    return nouns\n",
        "\n",
        "def book_features(book_name):\n",
        "    return {'book': book_name.lower()}\n",
        "\n",
        "def predict_author(book_name):\n",
        "    featureset = book_features(book_name)\n",
        "    return author_classifier.classify(featureset)\n",
        "\n",
        "\n",
        "\n",
        "df = pd.read_csv('books3.csv')\n",
        "\n",
        "author_labeled_data = [(book_features(row['Book']), row['Author']) for _, row in df.iterrows()]\n",
        "\n",
        "author_classifier = NaiveBayesFromScratch()\n",
        "author_classifier.train(author_labeled_data)\n",
        "\n",
        "\n",
        "print(\"Welcome! Ask who wrote a book, type 'exit' to quit.\\n\")\n",
        "\n",
        "while True:\n",
        "    user_input = input(\"Enter your query: \").strip().lower()\n",
        "    if user_input in [\"exit\", \"no thanks\", \"bye\"]:\n",
        "        print(\"Have a nice day! Goodbye!\")\n",
        "        break\n",
        "\n",
        "    tokens = word_tokenize(user_input)\n",
        "    processed_tokens = [token for token in tokens if token.isalnum()]\n",
        "    entity_words = extract_nouns(processed_tokens)\n",
        "    last_entity = entity_words[-1] if entity_words else None\n",
        "\n",
        "    if not last_entity:\n",
        "        print(\"Sorry, I couldn't understand your query.\")\n",
        "        continue\n",
        "\n",
        "    if \"author\" in user_input and last_entity in df['Book'].values:\n",
        "        author = predict_author(last_entity)\n",
        "        print(f\"The author of '{last_entity}' is: {author}\")\n",
        "    else:\n",
        "        print(f\"Sorry, no information available for '{last_entity}'.\")\n"
      ]
    }
  ]
}